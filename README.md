# online-retail-data-engineering
End-to-end data engineering pipeline using PySpark, PostgreSQL, and dbt
# Online Retail Data Engineering Project

End-to-end data engineering pipeline built using **PySpark**, **PostgreSQL**, and **dbt** to process, transform, and model retail transaction data into analytics-ready datasets.

---

## ğŸ“Œ Project Overview

This project demonstrates a complete **data engineering workflow**, starting from raw CSV ingestion to analytics-ready tables using modern data engineering tools and best practices.

Key objectives:
- Clean and transform large-scale retail data
- Design a dimensional data model (star schema)
- Load data into a PostgreSQL data warehouse
- Build dbt staging and mart models for analytics

---

## ğŸ—ï¸ Architecture

```text
Raw CSV Dataset
      |
      v
PySpark (Cleaning & Transformation)
      |
      v
PostgreSQL (Fact & Dimension Tables)
      |
      v
dbt (Staging + Mart Models)
      |
      v
Analytics / Reporting
ğŸ› ï¸ Tech Stack
PySpark â€“ large-scale data cleaning and transformations

PostgreSQL â€“ relational data warehouse

dbt â€“ data modeling, transformations, and testing

Python â€“ orchestration and scripting

GitHub â€“ version control and project management

ğŸ“‚ Project Structure
text
Copy code
online-retail-data-engineering/
â”œâ”€â”€ Online_Retail_Data_Engineering_Pipeline.ipynb
â”œâ”€â”€ README.md
â””â”€â”€ retail_dbt/
    â”œâ”€â”€ dbt_project.yml
    â””â”€â”€ models/
        â”œâ”€â”€ staging/
        â”‚   â”œâ”€â”€ sources.yml
        â”‚   â”œâ”€â”€ stg_dim_customer.sql
        â”‚   â”œâ”€â”€ stg_dim_product.sql
        â”‚   â”œâ”€â”€ stg_dim_date.sql
        â”‚   â””â”€â”€ stg_fact_orders.sql
        â””â”€â”€ marts/
            â””â”€â”€ fct_revenue_by_country.sql
ğŸ”„ Data Pipeline Steps
1. Data Ingestion
Source: Online Retail CSV dataset

Loaded into PySpark for distributed processing

2. Data Cleaning & Transformation (PySpark)
Removed null and invalid records

Handled malformed timestamps

Filtered negative quantities and prices

Deduplicated transactions

Generated derived columns (invoice date, total amount)

3. Data Warehouse Design
Implemented a star schema

Fact table: fact_orders

Dimension tables: dim_customer, dim_product, dim_date

Loaded cleaned data into PostgreSQL

4. dbt Modeling
Staging models for standardized transformations

Mart model for business-level analytics

Ensured analytics-ready datasets

ğŸ§ª Data Quality & Validation
Applied dbt best practices:

Clear separation of staging and marts

Consistent naming conventions

Ensured correctness of dimensional keys

Validated transformed data outputs

ğŸ“Š Example Analytics
Total revenue by country

Order volume trends over time

Customer purchasing behavior

Mart model:

fct_revenue_by_country

ğŸš€ How to Run (High-Level)
bash
Copy code
# Run PySpark pipeline to clean and load data
# Configure dbt profile locally (credentials not included)

dbt run
dbt test
ğŸ” Notes
Raw datasets are not included in this repository

dbt profiles and database credentials are excluded for security reasons

This project focuses on data engineering design and transformations

ğŸ¯ Key Learnings
Built an end-to-end data engineering pipeline from scratch

Applied dimensional modeling and data warehousing concepts

Gained hands-on experience with PySpark and dbt

Practiced clean project structuring and version control

ğŸ‘¤ Author
Sai Prakash Goud
Aspiring Data Engineer


ğŸ’¡ Future Enhancements
Add dbt tests (not_null, unique)

Add orchestration using Airflow

Add BI dashboards (Power BI / Tableau)
